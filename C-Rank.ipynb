{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk;\n",
    "import string\n",
    "import sys\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import sklearn.decomposition as dec\n",
    "import sklearn.manifold as manif\n",
    "import matplotlib.pyplot as plt\n",
    "import mpld3\n",
    "import scipy.sparse.linalg as linalg\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import itertools \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag, wordnet\n",
    "from summa import keywords\n",
    "from summa.summarizer import summarize\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from pybabelfy.babelfy import *\n",
    "from nltk.corpus import stopwords\n",
    "from pybabelnet.babelnet import BabelNet\n",
    "from unidecode import unidecode    \n",
    "from collections import OrderedDict\n",
    "#from nltk.stem import PorterStemmer\n",
    "from porterStemmer import PorterStemmer\n",
    "from math import log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openFile(fileName):\n",
    "    file = open(fileName,\"r\") \n",
    "    text = file.read() \n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseText(texto):\n",
    "    text = \"\"\n",
    "    punctuation = string.punctuation\n",
    "    punctuation = punctuation.replace('!','')\n",
    "    punctuation = punctuation.replace('.','')\n",
    "    punctuation = punctuation.replace('?','')\n",
    "    punctuation = punctuation.replace('-','')\n",
    "    translator = str.maketrans('', '', punctuation)\n",
    "    \n",
    "    for l in texto.splitlines():\n",
    "        printable = \"\".join(filter(str.isprintable,l))\n",
    "        text = text + '\\n' + printable.translate(translator)\n",
    "    text = unidecode(text)\n",
    "    text = text.replace('%','')\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    text = re.sub(r'  ', ' ', text)\n",
    "    splitted_text = [s.strip() for s in text.splitlines()]\n",
    "    new_splitted_text = []\n",
    "    punctuation = ['.','?','!']\n",
    "    new_splitted_text.append(\"\")\n",
    "    for line in text.splitlines():\n",
    "        if new_splitted_text[-1][-1:] not in punctuation or len(new_splitted_text[-1] + line) <= 3000: \n",
    "            new_splitted_text[-1] = new_splitted_text[-1] + ' ' + line\n",
    "        else:\n",
    "            new_splitted_text.append(line)\n",
    "    translator = str.maketrans('', '', \"?!.\")\n",
    "    for l in new_splitted_text:\n",
    "        l = l.translate(translator)\n",
    "    return new_splitted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frag(semantic_annotation, input_text):\n",
    "    start = semantic_annotation.char_fragment_start()\n",
    "    end = semantic_annotation.char_fragment_end()\n",
    "    return input_text[start:end+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def babelfy(lang, KEY, splitted_text):\n",
    "    babelapi = Babelfy()\n",
    "    bn = BabelNet(key)\n",
    "    paragraphs_annotations = []\n",
    "    paragraphs_text = []\n",
    "    paragraphs_code = []\n",
    "    \n",
    "    count = 0\n",
    "    for paragraph in splitted_text: #annotate each paragraph\n",
    "        words_annotations = []\n",
    "        words_text = []\n",
    "        words_code = []\n",
    "        semantic_annotations = babelapi.disambiguate(paragraph,lang,key,match=\"EXACT_MATCHING\",cands=\"TOP\",mcs=\"ON\",anntype=\"ALL\")\n",
    "        \n",
    "        for semantic_annotation in semantic_annotations: #exclude unused annotations (multiword expressions)\n",
    "            # primeira palavra ou  palavra atual comeca apos anteior\n",
    "            if len(words_annotations) == 0 or words_annotations[-1].char_fragment_end() < semantic_annotation.char_fragment_start():\n",
    "                words_annotations.append(semantic_annotation)\n",
    "                #words_text.append(bn.getSynsets(semantic_annotation.babel_synset_id())[0].senses[0].properties.fullLemma.lower())\n",
    "                words_text.append(frag(semantic_annotation,paragraph))\n",
    "                words_code.append(semantic_annotation.babel_synset_id())\n",
    "\n",
    "            # palavra atual comeca junto da palavra anterior\n",
    "            elif words_annotations[-1].char_fragment_start() == semantic_annotation.char_fragment_start():\n",
    "                del words_annotations[-1]\n",
    "                words_annotations.append(semantic_annotation)\n",
    "                del words_text[-1]\n",
    "                #words_text.append(semantic_annotation.babel_synset_id())[0].senses[0].properties.fullLemma.lower())\n",
    "                words_text.append(frag(semantic_annotation,paragraph))\n",
    "                del words_code[-1]\n",
    "                words_code.append(semantic_annotation.babel_synset_id())\n",
    "\n",
    "\n",
    "        paragraphs_annotations.append(words_annotations)\n",
    "        paragraphs_text.append(words_text)\n",
    "        paragraphs_code.append(words_code)\n",
    "        count = count + 1\n",
    "        print(count)\n",
    "        \n",
    "    return paragraphs_annotations, paragraphs_text, paragraphs_code    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveText(file, paragraphs_code):\n",
    "    file = open(file,\"w+\") \n",
    "    for p in paragraphs_code:\n",
    "        for w in p:\n",
    "            file.write(w + \"#\")\n",
    "        file.write(\"\\n\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## dicionario[palavra] = codigo\n",
    " ## dicionarioCode[codigo] = palavra\n",
    " ## peso[codigo] = peso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDicts(paragraphs_text, paragraphs_code):\n",
    "    dicionario={}\n",
    "    peso={}\n",
    "    dicionarioCode={}\n",
    "    for paragraph, codes in zip(paragraphs_text, paragraphs_code):\n",
    "        for word, code in zip(paragraph, codes):\n",
    "            if code not in peso or word not in dicionario:\n",
    "                dicionario[word] = code\n",
    "                peso[code] = 1\n",
    "                if code not in dicionarioCode:\n",
    "                    dicionarioCode[code] = word\n",
    "            else:\n",
    "                peso[code] = peso[code] + 1   \n",
    "    return dicionario, dicionarioCode, peso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20751874963942185"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(1 - log(3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.DiGraph()\n",
    "g.add_edge(1,2, weight=1)\n",
    "g.add_edge(1,3, weight=0.5+0.20751874963942185)\n",
    "g.add_edge(2,3, weight=1.5)\n",
    "g.add_edge(2,4, weight=0.20751874963942185)\n",
    "g.add_edge(3,4, weight=1.5)\n",
    "g.add_edge(3,5, weight=0.5+0.20751874963942185)\n",
    "g.add_edge(3,6, weight=0.20751874963942185)\n",
    "g.add_edge(4,5, weight=1)\n",
    "g.add_edge(4,6, weight=0.5)\n",
    "g.add_edge(4,7, weight=0.20751874963942185)\n",
    "g.add_edge(5,6, weight=1)\n",
    "g.add_edge(5,7, weight=0.5)\n",
    "g.add_edge(5,8, weight=0.20751874963942185)\n",
    "g.add_edge(6,7, weight=1)\n",
    "g.add_edge(6,8, weight=0.5)\n",
    "g.add_edge(6,9, weight=0.20751874963942185)\n",
    "g.add_edge(7,8, weight=1)\n",
    "g.add_edge(7,9, weight=0.5)\n",
    "g.add_edge(8,9, weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = nx.degree_centrality(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.25,\n",
       " 2: 0.375,\n",
       " 3: 0.625,\n",
       " 4: 0.625,\n",
       " 5: 0.625,\n",
       " 6: 0.75,\n",
       " 7: 0.625,\n",
       " 8: 0.5,\n",
       " 9: 0.375}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createGraph(peso, paragraphs_code, dist):\n",
    "    g = nx.DiGraph()\n",
    "    g2 = nx.DiGraph()\n",
    "    g3 = nx.DiGraph()\n",
    "    for code, weight in peso.items():\n",
    "        g.add_node(code, peso=weight)\n",
    "        g2.add_node(code, peso=weight)\n",
    "        g3.add_node(code, peso=weight)\n",
    "    for line in paragraphs_code:\n",
    "        i = 0\n",
    "        for word in line:\n",
    "            i = i + 1\n",
    "            j = 0\n",
    "            for word2 in line:\n",
    "                j = j + 1\n",
    "                if j - i < dist and j - i > 0:\n",
    "                    if g.has_edge(word, word2):\n",
    "                        #g[word][word2]['weight'] += 1/(j-i)\n",
    "                        g[word][word2]['weight'] += 1 - log(j-i,dist)\n",
    "                    else:\n",
    "                        if word != word2:\n",
    "                            #g.add_edge(word, word2, weight=float(1/(j-i)))\n",
    "                            g.add_edge(word, word2, weight=float(1 - log(j-i,dist)))\n",
    "                            #g.add_edge(word, word2, weight=float(log(dist/j-i,10)))\n",
    "                if j - i == 1:\n",
    "                    if g2.has_edge(word, word2):\n",
    "                        g2[word][word2]['weight'] += 1\n",
    "                    else:\n",
    "                        if word != word2:\n",
    "                            g2.add_edge(word, word2, weight=1)\n",
    "                if j - i == 2:\n",
    "                    if g3.has_edge(word, word2):\n",
    "                        g3[word][word2]['weight'] += 1\n",
    "                    else:\n",
    "                        if word != word2:\n",
    "                            g3.add_edge(word, word2, weight=1)\n",
    "    ## Normalize edge weights\n",
    "    #arcs = 0\n",
    "    #for n1, n2, w in list(g.edges(data='weight')):\n",
    "    #    arcs = arcs + w\n",
    "    #for a in list(g.edges(data='weight')):\n",
    "    #    g[n1][n2]['weight'] = g[n1][n2]['weight'] * 100 / arcs \n",
    "\n",
    "    return g, g2, g3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nodeRank(g,dicionarioCode):\n",
    "    #pr = nx.pagerank(g, weight='weight')\n",
    "    #pr = nx.eigenvector.eigenvector_centrality_numpy(g, weight='weight')\n",
    "    #pr = nx.closeness_centrality(g, distance='weight')\n",
    "    pr = nx.degree_centrality(g)\n",
    "    desc = sorted(pr.keys(), key=pr.get,reverse=False)\n",
    "    pageRank = {}\n",
    "    for p in pr:\n",
    "        pageRank[p] = pr[str(p)] \n",
    "    pageRank = sorted(pageRank.items(), key=lambda kv: (kv[1], kv[0]), reverse=True)\n",
    "    return pageRank\n",
    "\n",
    "    #desc = sorted(pr.keys(), key=pr.get,reverse=False)\n",
    "    #pageRank = {}\n",
    "    #for p in pr:\n",
    "    #    pageRank[dicionarioCode[p]] = pr[str(p)] \n",
    "    #pageRank = sorted(pageRank.items(), key=lambda kv: (kv[1], kv[0]), reverse=True)\n",
    "    #return pageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortGraphs(graphs):\n",
    "    newGraphs={}\n",
    "    for name, g in graphs.items():\n",
    "        if len(name) == 3:\n",
    "            newName = name[:-1] + \"0\" +name[-1] \n",
    "            newGraphs[newName] = g\n",
    "        else:\n",
    "            newGraphs[name] = g\n",
    "\n",
    "    newGraphs = OrderedDict(sorted(newGraphs.items()))\n",
    "\n",
    "    graphs={}\n",
    "    for name, g in newGraphs.items():\n",
    "        if name[2] == '0':\n",
    "            newName = name.replace(\"0\", '')\n",
    "            graphs[newName] = g\n",
    "        else:\n",
    "            graphs[name] = g\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveKeyWords(keyWords, graphName, resultFile):\n",
    "    file = open(resultFile,\"w+\") \n",
    "    stemmer=PorterStemmer()\n",
    "    for words, name in zip(keyWords, graphName):\n",
    "        file.write(name + \" : \")\n",
    "        #for word in [it[0] for it in nltk.pos_tag([item[0] for item in words]) if it[1] != 'VBN'][:15]:\n",
    "        for word, weight, weight2 in words[:15]:\n",
    "            parsedPhrase = ''\n",
    "            count = 0\n",
    "            for w in word.split(' '):\n",
    "                if w != '':\n",
    "                    parsedWord = stemmer.stem(w.lower())\n",
    "                    count = count + 1\n",
    "                    if count > 1:\n",
    "                        parsedPhrase = parsedPhrase + ' ' + parsedWord\n",
    "                    else:\n",
    "                        parsedPhrase = parsedWord\n",
    "\n",
    "            if(word != words[0][0]):\n",
    "                file.write(\",\" + parsedPhrase)\n",
    "            else:\n",
    "                file.write(parsedPhrase)\n",
    "        file.write(\"\\n\")\n",
    "    file.close()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalSort(value):\n",
    "    numbers = re.compile(r'(\\d+)')\n",
    "    parts = numbers.split(value)\n",
    "    parts[1::2] = map(int, parts[1::2])\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def takeSecond(elem):\n",
    "    return elem[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyPhrasesCompilation(keyWords, g, g2, dicionarioCode):\n",
    "    key2 = {code : value for code, value in keyWords}\n",
    "    \n",
    "    keyphrases_all = []\n",
    "    for k in keyWords[:20]:\n",
    "        for k2 in keyWords[:20]:\n",
    "            if k[0] != k2[0]:\n",
    "                if g2.has_edge(k[0], k2[0]):\n",
    "                    if g2.has_edge(k2[0], k[0]) and g2[k2[0]][k[0]]['weight'] > g2[k[0]][k2[0]]['weight']:\n",
    "                        weight = g.out_degree(k2[0], weight='weight') + g.in_degree(k[0], weight='weight')\n",
    "                        phrase = [k2[0] + ',' + k[0], g2[k2[0]][k[0]]['weight'], g[k2[0]][k[0]]['weight'] / weight]\n",
    "                    else:\n",
    "                        weight = g.out_degree(k[0], weight='weight') + g.in_degree(k2[0], weight='weight')\n",
    "                        phrase = [k[0] + ',' + k2[0], g2[k[0]][k2[0]]['weight'], g[k[0]][k2[0]]['weight'] / weight]\n",
    "                    if phrase not in keyphrases_all:\n",
    "                        keyphrases_all.append(phrase)\n",
    "                        \n",
    "                        \n",
    "    keyphrases_weight = [t[2] for t in keyphrases_all if t[1] >= 5]   # only phrases that appeared more than 2 times\n",
    "    keyphrases_weight_norm = [float(i)/sum(keyphrases_weight) for i in keyphrases_weight]\n",
    "    keyphrases = [t for t in keyphrases_all if t[1] >= 5]\n",
    "    \n",
    "    for kp, n in zip(keyphrases, keyphrases_weight_norm):\n",
    "        codes = kp[0].split(',')\n",
    "        #kp[1] = ((key2[codes[0]] + key2[codes[1]] ) / 2 ) / ((1-(n/(peso[0][codes[0]] * peso[0][codes[1]]))) / 1)\n",
    "        #kp[1] = ((key2[codes[0]] + key2[codes[1]] ) / 2 ) / ((1-(n)) / 1)\n",
    "        kp[1] = ((key2[codes[0]] + key2[codes[1]]) / 2) + n/2\n",
    "        #kp[1] = n\n",
    "        kp[0] = dicionarioCode[codes[0]] + ' ' + dicionarioCode[codes[1]]\n",
    "        \n",
    "    \n",
    "    keywords = [[dicionarioCode[k[0]], k[1], 1] for k in keyWords]\n",
    "    merged = keyphrases + keywords\n",
    "    merged.sort(key=takeSecond, reverse=True)\n",
    "        \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDirectory = \"/home/mauro/Downloads/testeSemeval\"\n",
    "outputFile = \"/home/mauro/Downloads/testeSemeval/teste.txt\"\n",
    "saveFile = \"/home/mauro/Downloads/testeSemeval/teste2.txt\"\n",
    "resultFile = \"/home/mauro/Downloads/testeSemeval/resultado.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"EN\"\n",
    "key = \"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = 35\n",
    "#dist  = float(\"inf\")\n",
    "#dist = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitted_text = []\n",
    "#dicionario = []\n",
    "#dicionarioCode = []\n",
    "#peso = []\n",
    "#paragraphs_annotations = []\n",
    "#paragraphs_text = []\n",
    "#paragraphs_code = []\n",
    "#graphName = []\n",
    "graphs = []\n",
    "graphs2 = []\n",
    "keyWords = []\n",
    "keyPhrases = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save KeyWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graphName' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-fd6bbbeb0180>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msaveKeyWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyPhrases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraphName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresultFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'graphName' is not defined"
     ]
    }
   ],
   "source": [
    "saveKeyWords(keyPhrases, graphName, resultFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nodeRank(g,dicionarioCode):\n",
    "    #pr = nx.pagerank(g, weight='weight')\n",
    "    #pr = nx.eigenvector.eigenvector_centrality_numpy(g, weight='weight')\n",
    "    #pr = nx.closeness_centrality(g, distance='weight')\n",
    "    #pr = nx.betweenness_centrality(g, weight='weight')\n",
    "    pr = nx.degree_centrality(g)\n",
    "    desc = sorted(pr.keys(), key=pr.get,reverse=False)\n",
    "    pageRank = {}\n",
    "    for p in pr:\n",
    "        pageRank[p] = pr[str(p)] \n",
    "    pageRank = sorted(pageRank.items(), key=lambda kv: (kv[1], kv[0]), reverse=True)\n",
    "    return pageRank\n",
    "\n",
    "    #desc = sorted(pr.keys(), key=pr.get,reverse=False)\n",
    "    #pageRank = {}\n",
    "    #for p in pr:\n",
    "    #    pageRank[dicionarioCode[p]] = pr[str(p)] \n",
    "    #pageRank = sorted(pageRank.items(), key=lambda kv: (kv[1], kv[0]), reverse=True)\n",
    "    #return pageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permite respostas iguais\n",
    "def saveKeyWords(keyWords, graphName, resultFile):\n",
    "    file = open(resultFile,\"w+\") \n",
    "    stemmer=PorterStemmer()\n",
    "    for words, name in zip(keyWords, graphName):\n",
    "        file.write(name + \" : \")\n",
    "        #for word in [it[0] for it in nltk.pos_tag([item[0] for item in words]) if it[1] != 'VBN'][:15]:\n",
    "        for word, weight, weight2 in words[:15]:\n",
    "            parsedPhrase = ''\n",
    "            count = 0\n",
    "            for w in word.split(' '):\n",
    "                if w != '':\n",
    "                    parsedWord = stemmer.stem(w.lower(), 0, len(w)-1)\n",
    "                    count = count + 1\n",
    "                    if count > 1:\n",
    "                        parsedPhrase = parsedPhrase + ' ' + parsedWord\n",
    "                    else:\n",
    "                        parsedPhrase = parsedWord\n",
    "\n",
    "            if(word != words[0][0]):\n",
    "                file.write(\",\" + parsedPhrase)\n",
    "            else:\n",
    "                file.write(parsedPhrase)\n",
    "        file.write(\"\\n\")\n",
    "    file.close()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permite respostas iguais\n",
    "def saveKeyWords(keyWords, graphName, resultFile):\n",
    "    file = open(resultFile,\"w+\") \n",
    "    stemmer=PorterStemmer()\n",
    "    for words, name in zip(keyWords, graphName):\n",
    "        file.write(name + \" : \")\n",
    "        #for word in [it[0] for it in nltk.pos_tag([item[0] for item in words]) if it[1] != 'VBN'][:15]:\n",
    "        for word, weight, weight2 in words[:15]:\n",
    "            parsedPhrase = ''\n",
    "            for wr in word.split(' '):\n",
    "                for w,i in zip(wr.split('-'),range(len(wr.split('-')))):\n",
    "                    if w != '':\n",
    "                        parsedWord = stemmer.stem(w.lower(), 0, len(w)-1)\n",
    "                        if i >= 1:\n",
    "                            parsedPhrase = parsedPhrase + '-' + parsedWord\n",
    "                        else:\n",
    "                            parsedPhrase = parsedPhrase + parsedWord\n",
    "                \n",
    "                parsedPhrase = parsedPhrase + ' '\n",
    "            parsedPhrase = parsedPhrase[:len(parsedPhrase)-1]\n",
    "            parsedPhrase = parsedPhrase.replace('  ',' ')\n",
    "            \n",
    "            if(word != words[0][0]):\n",
    "                file.write(\",\" + parsedPhrase)\n",
    "            else:\n",
    "                file.write(parsedPhrase)\n",
    "        file.write(\"\\n\")\n",
    "    file.close()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimina respostas iguais (piora resultado)\n",
    "\n",
    "def saveKeyWords(keyWords, graphName, resultFile):\n",
    "    file = open(resultFile,\"w+\") \n",
    "    stemmer=PorterStemmer()\n",
    "    for words, name in zip(keyWords, graphName):\n",
    "        file.write(name + \" : \")\n",
    "        #for word in [it[0] for it in nltk.pos_tag([item[0] for item in words]) if it[1] != 'VBN'][:15]:\n",
    "        keys = set()\n",
    "        for word, weight, weight2 in words[:30]:\n",
    "            parsedPhrase = ''\n",
    "            for wr in word.split(' '):\n",
    "                for w,i in zip(wr.split('-'),range(len(wr.split('-')))):\n",
    "                    if w != '':\n",
    "                        parsedWord = stemmer.stem(w.lower(), 0, len(w)-1)\n",
    "                        if i >= 1:\n",
    "                            parsedPhrase = parsedPhrase + '-' + parsedWord\n",
    "                        else:\n",
    "                            parsedPhrase = parsedPhrase + parsedWord\n",
    "                parsedPhrase = parsedPhrase + ' '\n",
    "            parsedPhrase = parsedPhrase[:len(parsedPhrase)-1]\n",
    "            parsedPhrase = parsedPhrase.replace('  ',' ')  \n",
    "            if parsedPhrase not in keys:\n",
    "                keys.add(parsedPhrase)\n",
    "\n",
    "                if(word != words[0][0]):\n",
    "                    file.write(\",\" + parsedPhrase)\n",
    "                else:\n",
    "                    file.write(parsedPhrase)\n",
    "        file.write(\"\\n\")\n",
    "    file.close()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defineEdgeWeight(B, G, g):\n",
    "    weightInB = 0.0\n",
    "    #weightOutB = 0.0\n",
    "    \n",
    "    for u in B:\n",
    "        for v in G:\n",
    "            if g.has_edge(u, v):\n",
    "                weightInB = weightInB + g[u][v]['weight'] \n",
    "          #  if g.has_edge(v, u):\n",
    "          #      weightOutB = weightOutB + g[v][u]['weight']\n",
    "    return weightInB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeNodes(g1,g2,dicionarioCode):\n",
    "    \n",
    "    stemmer=PorterStemmer()\n",
    "    #regra para união dos vértices (devem ter o mesmo radical)\n",
    "    same_stem = lambda u, v: (stemmer.stem(dicionarioCode[u].lower(), 0, len(dicionarioCode[u])-1) == stemmer.stem(dicionarioCode[v].lower(), 0, len(dicionarioCode[v])-1))\n",
    "    \n",
    "    #regra do peso das uniões (soma de todos os vértices)\n",
    "    node_weight = lambda G: {'peso' : (sum([g1.nodes[i]['peso'] for i in G]))}\n",
    "    node_weight2 = lambda G: {'peso' : (sum([g2.nodes[i]['peso'] for i in G]))}\n",
    "    \n",
    "    #regra do peso das arestas (soma de todas as arestas)\n",
    "    edges_weight = lambda B, G: {'weight' : (defineEdgeWeight(B, G, g1))}\n",
    "    edges_weight2 = lambda B, G: {'weight' : (defineEdgeWeight(B, G, g2))}\n",
    "    \n",
    "        \n",
    "    #merge nodes\n",
    "    gra1 = nx.quotient_graph(g1,same_stem,node_data=node_weight, edge_data=edges_weight)\n",
    "    gra2 = nx.quotient_graph(g2,same_stem,node_data=node_weight2, edge_data=edges_weight2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #atualiza nome dos vertices\n",
    "    mappingNodes = {}\n",
    "    for b in gra1.nodes():\n",
    "        higherNode = ''\n",
    "        higherWeight = 0\n",
    "        for n in b:\n",
    "            if g1.nodes[n]['peso'] > higherWeight:\n",
    "                higherWeight = g1.nodes[n]['peso']\n",
    "                higherNode = n\n",
    "        mappingNodes[b] = higherNode\n",
    "        \n",
    "    grafo1 = nx.relabel_nodes(gra1, mappingNodes)\n",
    "    \n",
    "    mappingNodes = {}\n",
    "    for b in gra2.nodes():\n",
    "        higherNode = ''\n",
    "        higherWeight = 0\n",
    "        for n in b:\n",
    "            if g2.nodes[n]['peso'] > higherWeight:\n",
    "                higherWeight = g2.nodes[n]['peso']\n",
    "                higherNode = n\n",
    "        mappingNodes[b] = higherNode\n",
    "        \n",
    "    grafo2 = nx.relabel_nodes(gra2, mappingNodes)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return grafo1,grafo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permite respostas iguais\n",
    "def saveKeyWords(keyWords, graphName, resultFile):\n",
    "    file = open(resultFile,\"w+\") \n",
    "    stemmer=PorterStemmer()\n",
    "    for words, name in zip(keyWords, graphName):\n",
    "        file.write(name + \" : \")\n",
    "        #for word in [it[0] for it in nltk.pos_tag([item[0] for item in words]) if it[1] != 'VBN'][:15]:\n",
    "        for word, weight, weight2 in words[:15]:\n",
    "            parsedPhrase = ''\n",
    "            for wr in word.split(' '):\n",
    "                for w,i in zip(wr.split('-'),range(len(wr.split('-')))):\n",
    "                    if w != '':\n",
    "                        parsedWord = stemmer.stem(w.lower(), 0, len(w)-1)\n",
    "                        if i >= 1:\n",
    "                            parsedPhrase = parsedPhrase + '-' + parsedWord\n",
    "                        else:\n",
    "                            parsedPhrase = parsedPhrase + parsedWord\n",
    "                \n",
    "                parsedPhrase = parsedPhrase + ' '\n",
    "            parsedPhrase = parsedPhrase[:len(parsedPhrase)-1]\n",
    "            parsedPhrase = parsedPhrase.replace('  ',' ')\n",
    "            \n",
    "            if(word != words[0][0]):\n",
    "                file.write(\",\" + parsedPhrase)\n",
    "            else:\n",
    "                file.write(parsedPhrase)\n",
    "        file.write(\"\\n\")\n",
    "    file.close()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDicts(paragraphs_text, paragraphs_code):\n",
    "    dicionario={}\n",
    "    peso={}\n",
    "    dicionarioCode={}\n",
    "    for paragraph, codes in zip(paragraphs_text, paragraphs_code):\n",
    "        for word, code in zip(paragraph, codes):\n",
    "            if code not in peso:\n",
    "                peso[code] = 1\n",
    "            else:\n",
    "                peso[code] = peso[code] + 1\n",
    "                \n",
    "            if word not in dicionario:\n",
    "                    dicionario[word] = code\n",
    "            if code not in dicionarioCode:\n",
    "                    dicionarioCode[code] = word\n",
    "            #elif len(dicionarioCode[code]) < len(word):\n",
    "            #    dicionarioCode[code] = word\n",
    "    return dicionario, dicionarioCode, peso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'paragraphs_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-b226f66cf4a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpeso\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateDicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagraphs_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparagraphs_code\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdicionario\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdicionarioCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'paragraphs_text' is not defined"
     ]
    }
   ],
   "source": [
    "dicionario = []\n",
    "dicionarioCode= []\n",
    "peso = []\n",
    "for i in range(100):\n",
    "    d, dC, p = createDicts(paragraphs_text[i],paragraphs_code[i])\n",
    "    dicionario.append(d)\n",
    "    dicionarioCode.append(dC)\n",
    "    peso.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicionario = {}\n",
    "for d in dicionarioCode:\n",
    "    dicionario.update(d)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = {}\n",
    "for d in dicionarioCode:\n",
    "    tf.update(d)\n",
    "    \n",
    "for t in tf:\n",
    "    tf[t] = 0\n",
    "    for text in paragraphs_code:\n",
    "        tInText = False\n",
    "        for p in text:\n",
    "            if t in p:\n",
    "                tInText = True\n",
    "                break\n",
    "        if tInText:\n",
    "            tf[t] += 1\n",
    "            \n",
    "for t in tf:\n",
    "    tf[t] = log(len(paragraphs_code) / tf[t])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyPhrasesCompilation(peso, keyWords, g, g2, dicionarioCode,lenght,totalWords):\n",
    "    key2 = {code : value for code, value in keyWords}\n",
    "    \n",
    "    keyphrases_all = []\n",
    "\n",
    "        \n",
    "    for k in keyWords[:lenght]:\n",
    "        for k2 in keyWords[:lenght]:\n",
    "            if g2.has_edge(k[0], k2[0]) and g2[k[0]][k2[0]]['weight'] >= int(totalWords / 1000) + 2:\n",
    "                if g2.has_edge(k2[0], k[0]) == False or g2[k2[0]][k[0]]['weight'] < g2[k[0]][k2[0]]['weight']:\n",
    "                    weight = g.out_degree(k[0], weight='weight') + g.in_degree(k2[0], weight='weight')\n",
    "                    phrase = [k[0] + ',' + k2[0], g2[k[0]][k2[0]]['weight'], g[k[0]][k2[0]]['weight'] / weight]\n",
    "                    if phrase not in keyphrases_all:\n",
    "                        keyphrases_all.append(phrase)\n",
    "\n",
    "    keyphrases_all = sorted(keyphrases_all, key=lambda kv: (kv[1], kv[0]), reverse=True) \n",
    "    keyphrases_weight = [t[2] for t in keyphrases_all]\n",
    "    keyphrases_weight_norm = [float(i)/sum(keyphrases_weight) for i in keyphrases_weight]\n",
    "    keyphrases = [t for t in keyphrases_all]\n",
    "    \n",
    "    for kp, n in zip(keyphrases, keyphrases_weight_norm):\n",
    "        codes = kp[0].split(',')\n",
    "\n",
    "        kp[1] = ((key2[codes[0]] + key2[codes[1]])) * n\n",
    "\n",
    "        kp[0] = dicionarioCode[codes[0]] + ' ' + dicionarioCode[codes[1]]\n",
    "        \n",
    "    soma = sum([v[1] for v in keyphrases])\n",
    "    keyphrases = [[k[0], 1 * k[1]/soma, k[2]] for k in keyphrases]   \n",
    "    keywords = [[dicionarioCode[k[0]], k[1], 1] for k in keyWords]    \n",
    "    merged = keyphrases[:6] + keywords\n",
    "\n",
    "    merged.sort(key=takeSecond, reverse=True)\n",
    "        \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "phrases = []\n",
    "graphs = []\n",
    "graphs2 = []\n",
    "#graphs3 = []\n",
    "dit = 3 #testar com 15\n",
    "words = []\n",
    "\n",
    "for i in range(100):\n",
    "    g, g2, g3 = createGraph(peso[i], paragraphs_code[i], dit)\n",
    "    #g, g2 = mergeNodes(g,g2,dicionarioCode[i])    \n",
    "    graphs.append(g)\n",
    "    graphs2.append(g2)\n",
    "    \n",
    "    keywords = nodeRank(graphs[i],dicionarioCode[i])    \n",
    "    \n",
    "    \n",
    "    #Exclui verbos no infinitivo\n",
    "    key2 = []\n",
    "    for k in keywords:\n",
    "        #if graphs2[i].nodes[k[0]]['peso'] > 1:\n",
    "        palavras = dicionarioCode[i][k[0]]\n",
    "        tokens = nltk.word_tokenize(palavras)\n",
    "        verb = False\n",
    "        for w in nltk.pos_tag(tokens):\n",
    "            if w[1][0] != 'N' and w[1][0] != 'J' and w[1][0] != 'V':\n",
    "                verb = True\n",
    "            #if w[1] == 'VB'\n",
    "            #    verb = True\n",
    "        if verb:\n",
    "            bla = [k[0], 0]\n",
    "            key2.append(bla)\n",
    "\n",
    "        else:\n",
    "            key2.append(k)\n",
    "             \n",
    "                \n",
    "    keywords = []\n",
    "    keywords = key2\n",
    "    keywords = sorted(keywords, key=lambda kv: (kv[1], kv[0]), reverse=True)     \n",
    "    \n",
    "    lenght = int(.13*len(keywords))\n",
    "    soma = sum([v[1] for v in keywords[:lenght]])\n",
    "    keywords = [[k[0], k[1]/soma] for k in keywords[:lenght]]    \n",
    "    keywords = [[k[0], (k[1]**(1/(len(dicionarioCode[i][k[0]].split(' ')))))] for k in keywords[:lenght]]\n",
    "    #keywords = [[k[0], (k[1]**(1/(len(re.split(' |-',dicionarioCode[i][k[0]])))))] for k in keywords[:90]]\n",
    "    \n",
    "    #keywords = [[k[0], k[1]*tf[k[0]]] for k in keywords[:lenght]] \n",
    "    \n",
    "    keywords = sorted(keywords, key=lambda kv: (kv[1], kv[0]), reverse=True)  \n",
    "    \n",
    "    \n",
    "    firstWords = []\n",
    "    totalWords = sum([len(i) for i in paragraphs_code[i]])\n",
    "    \n",
    "    \n",
    "    if totalWords > 1000:\n",
    "        threshold = int(totalWords * .18)\n",
    "    else:\n",
    "        threshold = totalWords\n",
    "    \n",
    "    for p in paragraphs_code[i]:\n",
    "        for w in p:\n",
    "            if len(firstWords) < threshold:\n",
    "                firstWords.append(w)\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    newKeywords = []\n",
    "    for k in keywords:\n",
    "        if k[0] in firstWords:\n",
    "            newKeywords.append(k)\n",
    "    keywords = newKeywords\n",
    "    \n",
    "    nodesToRemove = []\n",
    "    for n in g:\n",
    "        inKeywords = False\n",
    "        for k in keywords:\n",
    "            if k[0] == n:\n",
    "                inKeywords = True\n",
    "        if inKeywords == False:\n",
    "            nodesToRemove.append(n)\n",
    "            \n",
    "    for n in nodesToRemove:\n",
    "        g.remove_node(n)\n",
    "        g2.remove_node(n)\n",
    "        \n",
    "    \n",
    "    #soma = sum([v[1] for v in keywords])\n",
    "    #keywords = [[k[0], k[1]/soma] for k in keywords]    \n",
    "    #keywords = [[k[0], 3*(k[1]**(1/(len(dicionarioCode[i][k[0]].split(' ')))))] for k in keywords]\n",
    "    #for k in keywords:\n",
    "        #if len(dicionarioCode[i][k[0]].split(' ')) <= 1:\n",
    "        #    k[1] = 0\n",
    "    \n",
    "    \n",
    "    #Test with just 1 keyword\n",
    "    #for k in keywords:\n",
    "    #    if len(dicionarioCode[i][k[0]].split(' ')) > 1:\n",
    "    #        k[1] = 0\n",
    "    \n",
    "    \n",
    "   # keywords = [[k[0], 1*(k[1]*((len(dicionarioCode[i][k[0]].split(' ')))))] for k in keywords[:90]]\n",
    "    \n",
    "    #keywords = [[k[0], (k[1] + ((k[1] * (len(dicionarioCode[i][k[0]].split(' ')) - 1))**(1/3)))] for k in keywords]\n",
    "    #keywords = [[k[0], (k[1] + 1* (35/(0.000000000000001+sum([1 for t in keywords[:35] if len(dicionarioCode[i][t[0]].split(' ')) == len(dicionarioCode[i][k[0]].split(' '))]))) * k[1])] for k in keywords[:35]]\n",
    "    words.append(keywords)\n",
    "    keyphrase = keyPhrasesCompilation(peso[i], keywords,graphs[i],graphs2[i],dicionarioCode[i],lenght,totalWords)    \n",
    "    phrases.append(keyphrase)\n",
    "    print(i)\n",
    "saveKeyWords(phrases, graphName, '/home/mauro/Downloads/SemEval2010/test_answer/resultado.txt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['local area network', 0.21417348839739905, 1],\n",
       " ['Data Center', 0.18682012208131593, 1],\n",
       " ['management  system', 0.17575631692144592, 1],\n",
       " ['live migration', 0.09780422798312656, 1],\n",
       " ['server Migration', 0.07877944881582494, 0.21414670040441414],\n",
       " ['make use', 0.06821706337320382, 1],\n",
       " ['server virtualization', 0.06629506915612746, 1],\n",
       " ['service providers', 0.06227337402710532, 1],\n",
       " ['virtual server', 0.047478834786781166, 0.21642710119101485],\n",
       " ['Migration WANs', 0.04558466167391896, 0.16674724380359526],\n",
       " ['Migration', 0.04550155118924513, 1],\n",
       " ['server', 0.03903826266804554, 1],\n",
       " ['applications', 0.029472595656670136, 1],\n",
       " ['physical server', 0.024959349731658188, 0.1282429102980701],\n",
       " ['Internet-based', 0.024301964839710467, 1],\n",
       " ['services', 0.02042399172699071, 1],\n",
       " ['storage', 0.019906928645294746, 1],\n",
       " ['Approach', 0.019906928645294746, 1],\n",
       " ['data', 0.01809720785935886, 1],\n",
       " ['WANs', 0.017321613236814908, 1],\n",
       " ['replication', 0.015770423991727005, 1],\n",
       " ['technologies', 0.014736297828335068, 1],\n",
       " ['manner', 0.013702171664943136, 1],\n",
       " ['remote', 0.013185108583247167, 1],\n",
       " ['outages', 0.013185108583247167, 1],\n",
       " ['new', 0.0126680455015512, 1],\n",
       " ['operation', 0.012150982419855233, 1],\n",
       " ['case', 0.012150982419855233, 1],\n",
       " ['involved', 0.011633919338159266, 1],\n",
       " ['achieve', 0.011633919338159266, 1],\n",
       " ['virtual', 0.011375387797311282, 1],\n",
       " ['network', 0.011375387797311282, 1],\n",
       " ['mechanisms', 0.011116856256463298, 1],\n",
       " ['environment', 0.01059979317476733, 1],\n",
       " ['requires', 0.010082730093071363, 1],\n",
       " ['propose', 0.010082730093071363, 1],\n",
       " ['migrated', 0.009307135470527412, 1],\n",
       " ['system', 0.00904860392967943, 1],\n",
       " ['networking', 0.008790072388831446, 1],\n",
       " ['enable', 0.008790072388831446, 1],\n",
       " ['presents', 0.008790072388831446, 1],\n",
       " ['allows', 0.008014477766287494, 1],\n",
       " ['Context', 0.00775594622543951, 1],\n",
       " ['storage', 0.007238883143743544, 1],\n",
       " ['work', 0.006721820062047575, 1],\n",
       " ['downtime', 0.006721820062047575, 1],\n",
       " ['live', 0.006463288521199592, 1],\n",
       " ['events', 0.006463288521199592, 1],\n",
       " ['level', 0.006463288521199592, 1],\n",
       " ['subsystems', 0.005946225439503624, 1],\n",
       " ['failures', 0.005946225439503624, 1],\n",
       " ['described', 0.005946225439503624, 1],\n",
       " ['physical', 0.005687693898655641, 1],\n",
       " ['advocate', 0.005687693898655641, 1],\n",
       " ['used', 0.005170630816959673, 1],\n",
       " ['different', 0.005170630816959673, 1],\n",
       " ['available', 0.005170630816959673, 1],\n",
       " ['Second', 0.005170630816959673, 1],\n",
       " ['Design', 0.005170630816959673, 1],\n",
       " ['components', 0.005170630816959673, 1],\n",
       " ['ensure', 0.00491209927611169, 1],\n",
       " ['disks', 0.00491209927611169, 1],\n",
       " ['wide', 0.004653567735263706, 1],\n",
       " ['aware', 0.004395036194415723, 1],\n",
       " ['significant', 0.004136504653567739, 1],\n",
       " ['needed', 0.004136504653567739, 1],\n",
       " ['met', 0.004136504653567739, 1],\n",
       " ['nature', 0.004136504653567739, 1],\n",
       " ['access', 0.004136504653567739, 1],\n",
       " ['connectivity', 0.003877973112719755, 1],\n",
       " ['unplanned', 0.003877973112719755, 1],\n",
       " ['local', 0.003877973112719755, 1],\n",
       " ['hosted', 0.003877973112719755, 1],\n",
       " ['based', 0.003877973112719755, 1]]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveKeyWords(phrases, graphName, '/home/mauro/Downloads/SemEval2010/test_answer/resultado.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-85539f0746a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mphrases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "phrases[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-498b13a341b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mphrases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "phrases[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-871f720de9b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mindice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mphrases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtexto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "indice = 2\n",
    "for p in phrases[indice]:\n",
    "    tokens = nltk.word_tokenize(p[0])\n",
    "    texto = \"\"\n",
    "    for w in nltk.pos_tag(tokens):\n",
    "        print(w)\n",
    "    \n",
    "    #for word in tokens:\n",
    "    #    texto = texto + word + ' ' + \n",
    "        #palavras = dicionarioCode[i][k[0]]\n",
    "        #tokens = nltk.word_tokenize(palavras)\n",
    "        #for t in tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-a4aa4ee0f389>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdicionarioCode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for w in words[7]:\n",
    "    print(dicionarioCode[7][w[0]] + ' ' + str(w[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveKeyWords(keyWords, graphName, resultFile):\n",
    "    file = open(resultFile,\"w+\") \n",
    "    stemmer=PorterStemmer()\n",
    "    for words, name in zip(keyWords, graphName):\n",
    "        file.write(name + \" : \")\n",
    "        #for word in [it[0] for it in nltk.pos_tag([item[0] for item in words]) if it[1] != 'VBN'][:15]:\n",
    "        for word, weight, weight2 in words[:15]:\n",
    "            parsedPhrase = ''\n",
    "            count = 0\n",
    "            for w in word.split(' '):\n",
    "                if w != '':\n",
    "                    parsedWord = stemmer.stem(w.lower())\n",
    "                    count = count + 1\n",
    "                    if count > 1:\n",
    "                        parsedPhrase = parsedPhrase + ' ' + parsedWord\n",
    "                    else:\n",
    "                        parsedPhrase = parsedWord\n",
    "\n",
    "            if(word != words[0][0]):\n",
    "                file.write(\",\" + parsedPhrase)\n",
    "            else:\n",
    "                file.write(parsedPhrase)\n",
    "        file.write(\"\\n\")\n",
    "    file.close()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-da8d99249b39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\t'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdicionarioCode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\t'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k in words[7]:\n",
    "    count = count + 1\n",
    "    print(str(count) + '\\t' + dicionarioCode[3][k[0]] + '\\t' + str(k[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-b86d125f2bb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpeso\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bn:00819565n'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "peso[0]['bn:00819565n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-d7bea42cbff7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgraphs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bn:03664545n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "graphs[0].node('bn:03664545n', weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-3c58a87e03e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdicionarioCode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bn:03664545n'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "dicionarioCode[0]['bn:03664545n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-292d28a1641e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdicionario\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'UDDI'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "dicionario[0]['UDDI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-075a413ede99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgraphs2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bn:00041777n'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bn:00395755n'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "graphs2[0]['bn:00041777n']['bn:00395755n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-a3982e457840>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgraphs2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bn:00102206a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bn:00036197n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "graphs2[0].has_edge('bn:00102206a', 'bn:00036197n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-762233b538b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraphs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bn:00070651n'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdicionarioCode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for u, v, weight in graphs[0].edges(data='weight'):\n",
    "    if u == 'bn:00070651n' and weight > 5 and u != v:\n",
    "        print(dicionarioCode[0][v] + ' ' + str(weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Babelfy text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graphName = []\n",
    "splitted_text = []\n",
    "dicionario = []\n",
    "dicionarioCode = []\n",
    "peso = []\n",
    "paragraphs_annotations = []\n",
    "paragraphs_text = []\n",
    "paragraphs_code = []\n",
    "graphs = []\n",
    "graphs2 = []\n",
    "keyWords = []\n",
    "keyPhrases = []\n",
    "inputDirectory = \"/home/mauro/Downloads/SemEval2010/test/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startDownload():\n",
    "    count = 0\n",
    "    atual = 81\n",
    "    for filename in sorted(glob.glob(os.path.join(inputDirectory, '*.txt.final')), key=numericalSort):\n",
    "        if count >= atual:\n",
    "            graphName.append(filename[39] + '-' + str(re.findall(r'\\d+', filename[39:])[0]))\n",
    "\n",
    "            text = openFile(filename) \n",
    "            st = parseText(text)\n",
    "            splitted_text.append(st)\n",
    "\n",
    "            pa, pt, pc  = babelfy(lang, key, st)\n",
    "            paragraphs_annotations.append(pa)\n",
    "            paragraphs_text.append(pt)\n",
    "            paragraphs_code.append(pc)\n",
    "            #saveText(saveFile, paragraphs_code)\n",
    "\n",
    "            d, dc, p = createDicts(pt, pc)\n",
    "            dicionario.append(d)\n",
    "            dicionarioCode.append(dc)\n",
    "            peso.append(p)\n",
    "        count = count + 1\n",
    "        print('Artigo: ' + str(count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run download in background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr = threading.Thread(target=startDownload, args=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr.daemon = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr.is_alive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(splitted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(paragraphs_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitted_text.pop(69)\n",
    "#graphName.pop(69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save babelfyed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile = '/home/mauro/Documentos/Mestrado/babelfyLists/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''with open(saveFile + \"graphName.pickle\", \"wb\") as fp:\n",
    "    pickle.dump(graphName, fp)\n",
    "with open(saveFile + \"splitted_text.pickle\", \"wb\") as fp:\n",
    "    pickle.dump(splitted_text, fp)\n",
    "with open(saveFile + \"dicionario.pickle\", \"wb\") as fp:\n",
    "    pickle.dump(dicionario, fp)\n",
    "with open(saveFile + \"dicionarioCode.pickle\", \"wb\") as fp:\n",
    "    pickle.dump(dicionarioCode, fp)\n",
    "with open(saveFile + \"peso.pickle\", \"wb\") as fp:\n",
    "    pickle.dump(peso, fp)\n",
    "with open(saveFile + \"paragraphs_annotations.pickle\", \"wb\") as fp:\n",
    "    pickle.dump(paragraphs_annotations, fp)\n",
    "with open(saveFile + \"paragraphs_text.pickle\", \"wb\") as fp:\n",
    "    pickle.dump(paragraphs_text, fp)\n",
    "with open(saveFile + \"paragraphs_code.pickle\", \"wb\") as fp:\n",
    "    pickle.dump(paragraphs_code, fp)'''\n",
    "with open(saveFile + \"graphsDistINF.pickle\", \"wb\") as fp:\n",
    "    pickle.dump(graphs, fp)\n",
    "with open(saveFile + \"graphs2DistINF.pickle\", \"wb\") as fp:\n",
    "    pickle.dump(graphs2, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open babelfyed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphName = []\n",
    "splitted_text = []\n",
    "dicionario = []\n",
    "dicionarioCode = []\n",
    "peso = []\n",
    "paragraphs_annotations = []\n",
    "paragraphs_text = []\n",
    "paragraphs_code = []\n",
    "graphs = []\n",
    "graphs2 = []\n",
    "keyWords = []\n",
    "keyPhrases = []\n",
    "inputDirectory = \"/home/mauro/Downloads/SemEval2010/test/\"\n",
    "saveFile = '/home/mauro/Documentos/Mestrado/babelfyLists/'\n",
    "\n",
    "with open (saveFile + \"graphName.pickle\", 'rb') as fp:\n",
    "    graphName = pickle.load(fp)\n",
    "with open (saveFile + \"splitted_text.pickle\", 'rb') as fp:\n",
    "    splitted_text = pickle.load(fp)\n",
    "with open (saveFile + \"dicionario.pickle\", 'rb') as fp:\n",
    "    dicionario = pickle.load(fp)\n",
    "with open (saveFile + \"dicionarioCode.pickle\", 'rb') as fp:\n",
    "    dicionarioCode = pickle.load(fp)\n",
    "with open (saveFile + \"peso.pickle\", 'rb') as fp:\n",
    "    peso = pickle.load(fp)\n",
    "with open (saveFile + \"paragraphs_annotations.pickle\", 'rb') as fp:\n",
    "    paragraphs_annotations = pickle.load(fp)\n",
    "with open (saveFile + \"paragraphs_text.pickle\", 'rb') as fp:\n",
    "    paragraphs_text = pickle.load(fp)\n",
    "with open (saveFile + \"paragraphs_code.pickle\", 'rb') as fp:\n",
    "    paragraphs_code = pickle.load(fp)\n",
    "with open (saveFile + \"graphsDist4.pickle\", 'rb') as fp:\n",
    "    graphs = pickle.load(fp)\n",
    "with open (saveFile + \"graphs2Dist4.pickle\", 'rb') as fp:\n",
    "    graphs2 = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
